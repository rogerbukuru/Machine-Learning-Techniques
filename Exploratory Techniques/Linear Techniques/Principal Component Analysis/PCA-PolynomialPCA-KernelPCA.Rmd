---
title: "PCA, Polynomial PCA and Kernel PCA"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

- Projection of high-dimensional data into a lower-dimensional subspace by creating a
reduced set of linear transformations of the input variables, also called "feature
extraction".
- An eigenvalue-eigenvector problems but can also be viewed as special case of multivariate reduced rank regression.
- Aimed at finnding linear orthogonal combinations (projections) of the input variables
that are ordered by decreasing variance.
- Main aim is that of dimension reduction.
- Can also be used to discover important features of the data using graphical displays of the principal component scores.

Given the random r-vector $X=(X_{1},...,X_{r})^T$, with mean $u_x$ and covariance matrix $\Sigma_{XX}$, PCA seeks to replace the set 

# Classic PCA (Linear Technique)

- **PCA from eigen decomposition**

```{r}
rm(list = ls())
library(tidyverse)
library(dplyr)
library(rrr)
library(ggplot2)
library(scatterplot3d)
library(plotly)


data("iris")

iris_data = iris %>%
       as.tibble()  
head(iris)


x  = iris_data[,1:4]
# Center data
x_c = apply(x, 2, function(x)(x-mean(x)))
x_cov = cov(x)

eigen_decomp = eigen(x_cov)

eigenvectors = eigen_decomp$vectors # Principal Components
eigenvalues  = eigen_decomp$values
pca_scores   = t(eigenvectors%*%t(x_c)) %>% as.tibble()

pca_components_variance = (eigenvalues)/sum(eigenvalues)
pca_components_variance_df = data.frame(`PC` = 1:length(pca_components_variance), `Var Explained` = pca_components_variance) %>% as.tibble()

# Plot Scree Plot
ggplot(pca_components_variance_df, aes(x = PC, y = Var.Explained)) +
  geom_point() + # Add points
  geom_line() + # Connect points with lines
  xlab("Principal Component") +
  ylab("Proportion of Variance Explained") +
  ggtitle("Scree Plot") +
  theme_minimal() # Using a minimal theme for aesthetics

# Plot PCA Scores
ggplot(data = pca_scores, mapping = aes(x = V1, y = V2))+
      geom_point()
```

- **PCA using in built package, EigenDecomp and SVD**

- Built in package uses eigen value decomposition, so eigenvalues are variances
  - Actually returns the squared eigenvalues under $sdev$
- SVD decomposition $v$ is the eigenvectors and $d$ has the eigenvalues on it's diagonal, these need to be squared to get the variance

```{r}
rm(list = ls())
library(tidyverse)
library(dplyr)
library(rrr)
library(ggplot2)
library(scatterplot3d)
library(plotly)

data("iris")

iris_data = iris %>%
       as.tibble()  
head(iris)


X  = iris_data[,1:4]

classic_pca = prcomp(X, retx = TRUE, center = TRUE, scale. =  FALSE)
pca_components = classic_pca$rotation
pca_scores = classic_pca$x%>% # Linear combination of centered data and eigenvectors
             as.tibble()
pca_classic_variance_explained = (classic_pca$sdev^2)/sum(classic_pca$sdev^2)
pca_classic_variance_explained_df = data.frame(`PC` = 1:length(pca_classic_variance_explained), `Var Explained` = pca_classic_variance_explained) %>% as.tibble()

summary(classic_pca)
# Plot Scree Plot
ggplot(pca_classic_variance_explained_df, aes(x = PC, y = Var.Explained)) +
  geom_point() + # Add points
  geom_line() + # Connect points with lines
  xlab("Principal Component") +
  ylab("Proportion of Variance Explained") +
  ggtitle("Scree Plot") +
  theme_minimal() # Using a minimal theme for aesthetics

# Plot PCA Scores
ggplot(data = pca_scores, mapping = aes(x = PC1, y = PC2))+
      geom_point()
```

### Linear PCA Under RRRR


# Non-Linear PCA Techniques

- We have little visual guidance to help us identify any meaningful low-dimensional structure hidden in high-dimensional data.
- The linear projection methods are useful in discovering low-dimensional structure when the data actually lie in a linear (or approximately linear) lower-dimensional subspace (called a manifold $M$) of input space $R^d$
- But linear methods will not be able to capture the curvature of the data. What can we do if we know or suspect that the data actually lie on a low-dimensional nonlinear manifold, whose structure and dimensionality are both assumed unknown ?
- When a linear representation of the data is unsatisfactory, we turn to specialized methods designed to recover nonlinear structure.
  - identify the nonlinear manifold $M$

  

## Polynomial PCA

- Idea of generalizing PCA to a nonlinear case.
- We transform the set of input variables using a quadratic, cubic, or higher degree polynomial, and then apply linear PCA.
- The resulting polynomial PCA comes down to eigen analysis, by this time attention is focused on the value of the smallest few eigenvalues for nonlinear dimensionality reduction
- If for example we have bivariate data and the data follow an exact quadratic curve, the smallest eigenvalue of the covariance matrix of the extended vector will be zero and the score of the last principal component will be constant with a value of zero

### Polynomial PCA Steps

- Extend the input variables to an $s$ degree polynomial i.e increase the dimension of the input space
- From the extended input space perform linear PCA
  - The idea is that in a some input space, the data will be linear i.e the points, such that we can use a linear transformation within this space to reduce the dimension.


- **Example with no noise**

```{r}

rm(list = ls())
library(tidyverse)
library(dplyr)
library(rrr)
library(ggplot2)
library(scatterplot3d)
library(plotly)

n = 201
x1 = seq(from = -1.5, to = 0.5, by = 0.01)
x2 = 4*x1^2 + 4*x1 + 2
plot(x1,x2) # initial plot of the data (this is the initial input space, where we suspect that the relationship in the data is non-linear)

# We can start by performing standard linear PCA
original_data = data.frame(x1,x2)
linear_pca = prcomp(original_data, center = TRUE)
summary(linear_pca)

# We extend the data
data_transformed = data.frame(x1,x2, x1^2, x2^2, x1*x2) # quadratic approximation using polynomial PCA
svd_output = svd(cov(data_transformed))

principal_components = round(svd_output$v,3)
eigenvalues = round(svd_output$d,3)
# Observe that the 
principal_components
eigenvalues

pca.transformed = prcomp(data_transformed) # PCA on the var-cov matrix
summary(pca.transformed)
variance.transformed = round(pca.transformed$sdev^2,4)

## Loadings (PCA components)
loadings.transformed = round(pca.transformed$rotation,4)

## PCA scores
scores.transformed = pca.transformed$x
round(scores.transformed[5], 3)
pairs(scores.transformed[,1:4])

```

**From the above**

- The last component eigenvalue is equal to 0
- The hyperplane defined by zero eigenvalue (5th principle component) is
0 = -0.696$x_1$ + 0.174$x_2$ - 0.696$x_1^2$
$x_2 = 4x_1^2 + 4x_1$ which recovers the original quadratic curve
- Scatterplot matrix of the pairwise scores of the first four principal components from quadratic PCA. The last principal component has all its values equal to zero and is not displayed
- After performing the extension and then applying PCA, we note that the last few eigenvalues are small with the last being zero, which indicates that the original data lies on a non-linear space, and in this case a quadratic space.

**Interpretation of pairwise scatterplot matrix**

- C1 vs. C2: Shows a non-linear, almost parabolic relationship. This indicates that the first and second principal components are capturing some quadratic structure in the data.

- C1 vs. C3: Exhibits a similar non-linear relationship, suggesting that C3 also captures additional quadratic interactions not fully explained by C1.

- C1 vs. C4: This scatterplot shows a more complex, possibly intertwined structure, indicating even higher-order interactions captured by C4.

- C2 vs. C3: Displays another non-linear relationship, highlighting that C2 and C3 capture distinct but related aspects of the quadratic structure in the data.

- C2 vs. C4: The pattern here suggests a complex interaction, reinforcing the idea that C4 is capturing variance not fully explained by the first three components.

C3 vs. C4: Shows an intricate, possibly circular pattern, indicating that C3 and C4 are capturing unique quadratic effects in the data.

- Example with noise

```{r}

rm(list = ls())
library(tidyverse)
library(dplyr)
library(rrr)
library(ggplot2)
library(scatterplot3d)
library(plotly)

set.seed(5)
n = 201
x1 = seq(from = -1.5, to = 0.5, by = 0.01)
x2 = 4*x1^2 + 4*x1 + 2

# Suppose now that our original data had noise

z = rnorm(n,0, 1)
x1_noisy = x1 + z
x2_noisy = x2 + z
plot(x1_noisy,x2_noisy)

data_transformed_noisy = data.frame(x1_noisy, x2_noisy, x1_noisy^2, x2_noisy^2, x1_noisy * x2_noisy)

pca.transformed_noisy = prcomp(data_transformed_noisy) # PCA on the var-cov matrix
summary(pca.transformed_noisy)
variance.transformed_noisy = round(pca.transformed_noisy$sdev^2,4)
variance.transformed_noisy

## Loadings (PCA components)
loadings.transformed = round(pca.transformed_noisy$rotation,4)
loadings.transformed

## PCA scores
scores.transformed_noisy = round(pca.transformed_noisy$x,4)

pairs(scores.transformed_noisy[,1:5])

```

**We observe the following**

- The eigenvalues are each greater than the respective eigenvalues from the noisless case, with the smallest eigenvalue now 0.0303 (this could be different for you unless if you use set.seed()). 
- As we would expect some of the well-defined patterns in the scatterplot matrix become blurred in the noisy case.
- Even if we significantly reduce the variance of the added noise component, the results of the quadratic PCA will be strongly affected by the noisiness of the data

What are some challenges with polynomial PCA ?
- Standardization of all extended data matrix may be desirable
- The size of the extended vector for quadratic PCA increases quickly with increasing dimensions and for high degrees of polynomial
- Therefore sample size has to be much larger than the dimensionality of the extended data matrix.
  - If this is not the case we can face the following issues
    - Effect bias variance tradeoff, if violated, we will have low bias in-sample data due to overfitting and high variance with out of sample data 
    - In high dimension, data tend to become sparse, making it difficult to find meaningful structures or patterns.
    
## Kernel PCA

- An approach that also generalizes polynomial PCA is given by Kernal PCA
- This is an application of so called kernel methods, which are seen in SVMs.
- Via kernel PCA we perform nonlinear mapping that transforms the data onto a higher-dimensional space and use standard PCA in this higher-dimensional space to project the data back onto a lower dimensional space.
- Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original space
- A good tutorial can be found [here](https://atiulpin.wordpress.com/2015/04/02/a-tutorial-on-kernel-principal-component-analysis/)
- Our aim is to find a nonlinear feature mapping $\varphi: R^D -> F$ where $F$ is an extended feature space
  - Finding this mapping is not always feasible, because the number of such feature mappings is infinite
  - Kernel methods allows us to find theses mappings without any search or explicit definition of this linear mapping $\varphi(\cdot)$ , but work in a new feature space $F$ 
- To extend PCA and enable PCA to classify nonlinear data, we nonlinearly transform our input space $x_i\in R^p$ into a point $\phi(x_i)$ in the M-dimensional feature space $H$ where usually $M >> p$

- We define the kernel as a function of inner products within the transformed space $k(x_i,x_i)=<\phi(x_i), \phi(x_i> = \phi(x_i)^T\phi(x_i)$ 
  - which computes the inner products in the transformed space that quantifies the similarity of two observations. $k()$ is some        function that we will refer to as Kernel.
  - Kernel must be equivalent to an inner product in some feature space or specifically a kernel defines similarity of two observations at the transformed space
  - We need not explicitly construct the new feature space.
  - However we first need to understand what are inner products
    - An inner product is a mathematical operation on a vector space that takes two vectors and returns a scalar let $u,v \in V$ $\langle u, v \rangle$. It has the following properties
      - Symmetric $\langle u, v \rangle = \langle v, u \rangle$
      - Linear on the first argument. $\langle au + bv, w \rangle = a\langle u, w \rangle + b\langle v, w \rangle$
      - Positive Definiteness $\langle u, v \rangle \ge 0$ 
    - Applications of inner product
      - Angle between vectors
      - Orthogonality
      - Projections
  - So the use of using a Kernel is that it amounts to the fact that using kernels, one need only compute $k(x_i,x_i)$ for all          $C(n,2)$ distinct pairs of $i$ and $i'$.
    - This can be done without explicitly working in the enlarged feature space/defining the new set of features.

**Steps for kernel PCA**

- Pick a kernel function
- Construct the kernel matrix from the training data $x_i$
- Compute the Gram matrix $\tilde{K} = HKH$ where $H= \mathbf{1}_n-\frac{1}{n}\mathbf{J}_n$ with $\mathbf{J}_n=\mathbf{1}_n\mathbf{1}_n^T$ (done when the projected matrix does not have a mean of zero), this is known as centering
- Use $K\alpha = n\lambda\alpha$ to solve for the $\alpha$ using SVD
- Compute the kernel principal components $y(\mathbf{x}) = \Phi(\mathbf{x})^T \mathbf{v} = \sum_{i=1}^{n} \alpha_i \kappa(\mathbf{x}, \mathbf{x}_i)$
  - This involves solving the eigen problem and then projecting onto the eigenvectors.

**Challenges with kernel PCA**

- Searching for the proper kernel is a difficult task, and therefore use of KPCA is limited. However, in some practical applications, good candidates for kernel can be found from the nature of the problem.
- The dimension of the feature space, depending on the kernel, can be very high and it may be computationally prohibitive to compute the principal components in the feature space. (Computationally intractable)
- PCA's covariance matrix scales with the number of input dimension n.
- KPCA kernel matrix scales with the number of datapoints d.

**Kernel Functions**

- For a function to be a kernel map it must be valida meaning that it can be shown that the function computes a dot product of $\phi$ in the high dimensional space.

**Example**

```{r}
rm(list = ls())
library(tidyverse)
library(dplyr)
library(rrr)
library(ggplot2)
library(scatterplot3d)
library(plotly)
library(kernlab)

data("iris")

test = sample(1:150, 20)
#kpca = kpca(., data=iris[-test, -5], kernel = "rbfdot", kpar=list(sigma=0.2, features = 2))
#pcv(kpca)
#plot(rotated(kpc))

```

# Practice Exercises

## Linear PCA Exercices

```{r}
rm(list = ls())
library(ggplot2)
library(tidyverse)
data("iris")


iris_data = iris

head(iris_data)
x = as.matrix(iris_data[,-5])
x_c = scale(x, center=TRUE)

# PCA From First Principles using eigenvalue decomp

cov_x_c = cov(x_c)
evd = eigen(cov_x_c)
pca_components = evd$vectors
pca_scores = x_c%*%pca_components
colnames(pca_scores) = paste("PC", sep="", 1:ncol(pca_scores))
var_explained = round(evd$values / sum(evd$values), 4)
var_explained


pca_tbl = pca_scores%>%
          as_tibble()

ggplot(pca_tbl, aes(x=PC1, y=PC2))+
        geom_point(size = 3)
pairs(pca_scores)
```

- Observing the the set of PC scores in the matrix plot, we note that between the first two principal components, there is a clear clustering seperating the species, it seems as though there are only two clusters, but upon closer observations we note a third cluster, this ties in with the number of distinct species that exists within the data set. Between (PC1,PC3) and (PC1,PC3) we observe similar patterns. However between (PC2, PC3), (PC2, PC4) and (PC3, PC4) there is no clustering and a different behaviour is observed.

- Pairs with normal `prcomp`

```{r}
pca_analysis = prcomp(x_c, center=FALSE)
pca_scores = pca_analysis$x
summary(pca_analysis)
pairs(pca_scores)
```

- PCA under RRR approach from first principals

```{r}
# We define the matrix C = AB where A = V, B = t(V)

mux = apply(x, 2, mean)
V = evd$vectors
A = V
B = t(V)
n = nrow(V)

# Rank 1
A1 = matrix(A[, 1], ncol = 1, byrow = TRUE)
B1 = matrix(B[1,], ncol = n, byrow = TRUE)
C1 = A1%*%B1
x1 = C1%*%t(x_c) # => tcrossprod(x_c,C1)
mse_1 = sum((x_c-t(x1))^2)

# Rank 2
A2 = matrix(A[, 1:2], ncol = 2, byrow = TRUE)
B2 = matrix(B[1:2,], ncol = n, byrow = TRUE)
C2 = A2%*%B2
x2 = C2%*%t(x_c) # => tcrossprod(x_c,C2)
mse_2 = sum((x_c-t(x2))^2)

# Rank 3
A3 = matrix(A[, 1:3], ncol = 3, byrow = TRUE)
B3 = matrix(B[1:3,], ncol = n, byrow = TRUE)
C3 = A3%*%B3
x3 = C3%*%t(x_c) # => tcrossprod(x_c,C3)
mse_3 = sum((x_c-t(x3))^2)

# Full Rank
C = A%*%B
x_full = C%*%t(x_c) # => tcrossprod(x_c,C)
mse_full = sum((x_c-t(x_full))^2)

mse_1
mse_2
mse_3
mse_full
pairs(t(x_full))



```

- From RRR, we note that we achieve the lowest MSE when using the full rank coefficient matrix in this case. And the PCA scores in the matrix plot are consistent with the previous approaches.

- PCA using RRR package

```{r}
library(rrr)
pca_rrr = rrr(x_c, x_c, type = "pca", rank = 4, k=0)
pca_components_rrr = as.matrix(pca_rrr$PC)
pca_scores = x_c%*%pca_components_rrr
summary(pca_rrr)
pairs(pca_scores)

```


## Kernel PCA
Exercise 16.4: Kernel PCA
Using the pendigits data (in resources folder as a csv file), which consists of 10 992 handwritten digits, 

- (1) carry out an ordinary linear PCA

```{r}

rm(list=ls())
library(ggplot2)
library(tidyverse)

pendigits_data = read_csv("pendigits_csv.csv", col_names = TRUE)
x = pendigits_data[,-17]
x_c = scale(x, center = TRUE)
cov_x_c = cov(x_c)

eigen_decomp = eigen(cov_x_c)

pca_scores = x_c%*%eigen_decomp$vectors
var_explained = round(eigen_decomp$values/sum(eigen_decomp$values),4)

var_explained_tbl = data.frame(PC=1:ncol(pca_scores), var_explained = var_explained)

#summary(prcomp(x_c, center = FALSE))

ggplot(var_explained_tbl, aes(x=PC, y=var_explained))+
      geom_point()+
      geom_line()+
      labs(
        x = "Principal Component",
        y = "Proportion of Variance Explained",
        title = "Scree Plot") +
     theme_minimal() # Using a minimal theme for aesthetics

var_first_4_pca = sum(var_explained_tbl$var_explained[1:5])
#var_first_11_pca = sum(var_explained_tbl$var_explained[1:11])

var_first_4_pca
#var_first_11_pca

# First 11 PCA can explain 95.8% of the variance 

var_first_4_pca = data.frame(pca_scores[,1:4])

ggplot(var_first_4_pca, aes(x=X1, y=X2))+
      geom_point()+
      labs(
        x = "PC1",
        y = "PC2",
        title = "PCA Scores") +
     theme_minimal() # Using a minimal theme for aesthetics

```

(2) compute the kernel PC scores and plot them for different values of o for the RBF kernel. Use different colors for the 10 digits.
Comment on your findings and compare the PCA results to the kPCA results.

```{r}
library(kernlab)
library(tidyverse)

x_c = x_c%>%
      as_tibble()

#kpca = kpca(~., data = x_c, kernel= "rbfdot", kpar= list(sigma = 0.1), features #= 4)
load("kpcaResultsPenDigits.RData")

pca_data = pcv(kpca)%>%
           as_tibble() %>%
           mutate(class = as.factor(pendigits_data$class))

ggplot(pca_data, aes(x = V1, y = V2, color = as.factor(class))) +
  geom_point(size = 3) +
  labs(title = "Kernel PCA using RBF Kernel", x = "PC1", y = "PC2", color="Numbers") +
  theme_minimal()


ggplot(pca_data, aes(x = V1, y = V2, color = as.factor(class))) +
  geom_point(size = 3) +
  labs(title = "Kernel PCA using RBF Kernel", x = "PC1", y = "PC2", color="Numbers") +
  theme_minimal()

#scatterplot3d::scatterplot3d(pca_data, x = V1, y = V2, z = V3)

```

```{r}
rm(list=ls())

load(file = "coil20.Rda")
head(coil20)

x_c = scale(coil20, center= TRUE)
x_c = x_c%>%
      as_tibble()
#cov_x_c = cov(x_c)
#evd = eigen(cov_x_c)
#var_explained = evd$values
#pca_scores = x_c%*%evd$vectors

kpca(~., data = x_c, kernel = "rbfdot", )

```

# Appendix

## LLM Finetuning

- Check if inputs are correlated before performing PCA
- Check if PCA is appropriate for the data, by ana twolysis the first few principal components  in terms of linearity
- Standardize data by centering and scaling (unit variance)
- If the data seems non-linear, we can transform it by extending the feature space and then performing linear PCA. We don't know from off the bat if the data is non-linear, so we have to perform various dimension reduction techniques and assess performance of each method. 
  - So start with linear if not sure or don't have knowledge that data is not inherently linear, then benchmark this with other         non-linear techniques and assess performance. 
  - For polynomial PCA look at the results of the smallest eigenvalues, we want to observe that the values tappers to zero, in order to determine if an underlying non-linear pattern is being detected. However becareful when performing polynomail PCA as it can be     computationally heavy if we have a large input space!!
- Kernel PCA to be explored if we can determine a kernel function to work with, but can be good to represent non-linear data. Depending on kernel the high-dimensional space can be computationally expensive.
- Hyperparamter tuning of kernel parameters

## Kernel PCA from First Principals

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyverse)

# Load or create the pendigits data
data(pendigits)  # Ensure pendigits data is available in your environment
pendigits_data <- pendigits[, -ncol(pendigits)]  # Exclude the label column for kernel calculation
labels <- pendigits[, ncol(pendigits)]  # Extract the label column

# Step 1: Standardize the data
x_c <- scale(pendigits_data, center = TRUE)

# Step 2: Construct the Kernel Matrix using RBF kernel
sigma <- 0.1
rbf_kernel <- function(x, y, sigma) {
  exp(-sum((x - y)^2) / (2 * sigma^2))
}

n <- nrow(x_c)
K <- matrix(0, n, n)
for (i in 1:n) {
  for (j in 1:n) {
    K[i, j] <- rbf_kernel(x_c[i, ], x_c[j, ], sigma)
  }
}

# Step 3: Center the Kernel Matrix
one_n <- matrix(1 / n, n, n)
H <- diag(n) - one_n
K_centered <- H %*% K %*% H

# Step 4: Solve for alpha using eigen decomposition
eig <- eigen(K_centered)
values <- eig$values
vectors <- eig$vectors

# Step 5: Compute the Kernel Principal Components
# Select the top k components (e.g., k = 2)
k <- 2
alphas <- vectors[, 1:k]
principal_components <- K_centered %*% alphas

# Create a data frame for plotting
pc_df <- data.frame(PC1 = principal_components[, 1], PC2 = principal_components[, 2], Label = labels)

# Plot the first two principal components
ggplot(pc_df, aes(x = PC1, y = PC2, color = as.factor(Label))) +
  geom_point(size = 3) +
  labs(title = "Kernel PCA using RBF Kernel", x = "PC1", y = "PC2") +
  theme_minimal()

```
