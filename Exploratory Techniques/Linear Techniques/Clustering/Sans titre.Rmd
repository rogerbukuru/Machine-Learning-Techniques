---
title: "Cluster Analysis"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

- Purpose: allocate item to groups

- Key characteristic: unsupervised
  - "Unsupervised" in the sense that we do not know which observations belong to groups
  - Can be used for feature extraction
    - This does not allocate observations to groups, but rather creates new varianles from the original x

What is a cluster ?

- A cluster is a set of items that are grouped together according to some criterion such as:
  - Proximity e.g k-means clustering, MDS
  - points with a lower dissimilarity are grouped together.A point (1, 0) has a lower Euclidean distance to a point (0, 1) than a        point (5, 5), and so would be clustered with the (0, 1) point.
  - Probability
    - points allocated to probability distribution most likely to have generated them.
    Suppose point arose from either a N(0, 1) distribution or an N(5, 1) distribution. A point â€“1 would be allocated to cluster whose     distribution matches the first.
  - Modality
    - Imagine 100 observations of one variable. If we plot the density of that variable and it has two peaks, then we could divide         the data into two clusters.
  - Density
    - In a given dataset, 70% of observations lie between 0 and 1 and the remaining 30% lie between 5 and 6. The regions [0, 1] and        [5, 6] are high-density regions, and points lying within those regions are clustered together.

- A cluster is generally thought of as a group of items (object, points) in which each item is "close" (in some appropriate sense) to a central item of a cluster and that members of different clusters are "far away" from each other.

There are numerous ways of clustering a data set of n independent measurements on each of r correlated variables 
- Clustering Observations: Mapping individual samples to a groups. The K groups is unknown and has to be determined from data
- Clustering variables: Partition the $p$ variables into $K$ distinct groups, where the number K is unknown and has to be determied from the data.
- Two-way clustering: Simultaneosuly cluster by observations and variables. Other names "block clustering", "direct-clustering", "biclustering", or "co-clustering".

## A few Clustering approaches

- Hierarchical
  - Agglomerative
    - Begin with each observation its own cluster, and merge clusters until one cluster
  - Divisive
    - Begin with one cluster, split until each observation is its own cluster.
- Partitioning
  - Distance-based: k-means, PAM, FANNY
  - Probability-based: clustering using Gaussian mixtures
  
### Hierarchical Clustering

- We tend to use a dendogram to determine the number of clusters
- Idea of Single Linkage, Complete Linkage and Average Linkage
  - Cutting dendogram at a particular height leads to partitioning into a specified number of groups
  - Height of linkage refers to similarity of objects, the lower the height of merge, the more similar the objects
  - Mathematical algorithm described in notes.
  
#### Example 1

```{r}

```
### Combinatorial Algorithms

- Characterized by a many-to-one mapping or encoder $k=c(i)$ that assigns the ith observation to the k-th cluster 
- $c(i)$ is determined by finding the one that will minimize a "loss" function 
- A natural loss functions is what we refer to as the "within-cluster" distance $W(c)= T-B(c)$

    